"""
–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è Fine-Tuning –º–æ–¥–µ–ª–µ–π
"""
import gradio as gr
import os
import json
import threading
from trainer_module import train_model
from memory_estimator import (
    estimate_model_memory,
    estimate_training_memory,
    get_available_memory,
    check_memory_sufficiency,
)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel


# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
training_progress = []
training_status = {"running": False, "result": None}

# –ö—ç—à –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
inference_cache = {}


def estimate_memory_requirements(
    model_name,
    quantization_bits,
    batch_size,
    max_length,
    gradient_accumulation_steps,
    use_gradient_checkpointing,
    use_8bit_optimizer
):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏"""
    if not model_name:
        return "‚ö†Ô∏è –£–∫–∞–∂–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏"
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º quantization_bits
        qb = None if quantization_bits == "–ù–µ—Ç" else int(quantization_bits)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏
        model_memory = estimate_model_memory(model_name, qb)
        
        if 'error' in model_memory:
            return f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {model_memory['error']}"
        
        # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        training_memory = estimate_training_memory(
            model_memory['model_memory_gb'],
            batch_size,
            max_length,
            gradient_accumulation_steps,
            use_gradient_checkpointing,
            use_8bit_optimizer
        )
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
        available_memory = get_available_memory()
        
        if available_memory['available_gb'] == 0:
            memory_info = "‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ."
        else:
            memory_check = check_memory_sufficiency(
                training_memory['total_memory_gb'],
                available_memory['available_gb']
            )
            
            memory_info = f"""
üìä **–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏:**

**–ú–æ–¥–µ–ª—å:**
- –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~{model_memory['num_params']/1e9:.2f}B
- –ü–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏: {model_memory['model_memory_gb']} GB

**–û–±—É—á–µ–Ω–∏–µ:**
- –ú–æ–¥–µ–ª—å: {training_memory['model_memory_gb']} GB
- –ê–∫—Ç–∏–≤–∞—Ü–∏–∏: {training_memory['activation_memory_gb']} GB
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã: {training_memory['gradient_memory_gb']} GB
- –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {training_memory['optimizer_memory_gb']} GB
- –î–∞–Ω–Ω—ã–µ: {training_memory['data_memory_gb']} GB
- Overhead: {training_memory['overhead_gb']} GB
- **–ò–¢–û–ì–û: {training_memory['total_memory_gb']} GB**

**–î–æ—Å—Ç—É–ø–Ω–æ:**
- GPU: {available_memory['device_name']}
- –í—Å–µ–≥–æ –ø–∞–º—è—Ç–∏: {available_memory['total_gb']} GB
- –î–æ—Å—Ç—É–ø–Ω–æ: {available_memory['available_gb']} GB
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {available_memory['allocated_gb']} GB

**{memory_check['recommendation']}**
"""
        
        return memory_info
        
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø–∞–º—è—Ç–∏: {str(e)}"


def progress_callback(message):
    """Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
    training_progress.append(message)
    print(message)  # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å


def start_training(
    model_name,
    dataset_file,
    output_dir,
    adapter_path,
    continue_adapter,
    max_length,
    quantization_bits,
    use_double_quant,
    batch_size,
    gradient_accumulation_steps,
    learning_rate,
    num_train_epochs,
    lora_r,
    lora_alpha,
    lora_dropout,
    save_steps,
    logging_steps,
    use_gradient_checkpointing,
    use_8bit_optimizer,
):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    global training_status, training_progress
    
    if training_status["running"]:
        return "‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ!"
    
    if not model_name:
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏"
    
    if dataset_file is None:
        return "‚ùå –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç"

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º quantization_bits –≤ —á–∏—Å–ª–æ
    qb = None if quantization_bits == "–ù–µ—Ç" else int(quantization_bits)

    # –ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä—É (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º)
    adapter_path = adapter_path.strip() if isinstance(adapter_path, str) else ""
    if not continue_adapter:
        adapter_path = None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    dataset_path = dataset_file.name if hasattr(dataset_file, "name") else dataset_file
    
    training_status["running"] = True
    training_progress = []
    
    def train_thread():
        global training_status
        try:
            result = train_model(
                model_name=model_name,
                dataset_path=dataset_path,
                output_dir=output_dir if output_dir else None,
                adapter_path=adapter_path,
                max_length=int(max_length),
                quantization_bits=qb,
                use_double_quant=use_double_quant,
                batch_size=int(batch_size),
                gradient_accumulation_steps=int(gradient_accumulation_steps),
                learning_rate=float(learning_rate),
                num_train_epochs=int(num_train_epochs),
                lora_r=int(lora_r),
                lora_alpha=int(lora_alpha),
                lora_dropout=float(lora_dropout),
                save_steps=int(save_steps),
                logging_steps=int(logging_steps),
                use_gradient_checkpointing=use_gradient_checkpointing,
                use_8bit_optimizer=use_8bit_optimizer,
                progress_callback=progress_callback
            )
            training_status["result"] = result
        except Exception as e:
            training_status["result"] = {
                'success': False,
                'error': 'other',
                'message': str(e)
            }
        finally:
            training_status["running"] = False
    
    thread = threading.Thread(target=train_thread)
    thread.start()
    
    return "üöÄ –û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! –°–ª–µ–¥–∏—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –ª–æ–≥–∞—Ö."


def load_inference_model(base_model_name, adapter_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç (–∏–ª–∏ –±–µ—Ä—ë—Ç –∏–∑ –∫—ç—à–∞) –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∞–¥–∞–ø—Ç–µ—Ä–æ–º
    """
    key = (base_model_name, adapter_path)
    if key in inference_cache:
        return inference_cache[key]

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è GPU.")

    # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )

    if adapter_path and os.path.isdir(adapter_path):
        model = PeftModel.from_pretrained(model, adapter_path)

    model.eval()

    inference_cache[key] = (tokenizer, model)
    return tokenizer, model


def run_inference(
    base_model_name,
    adapter_path,
    prompt,
    max_new_tokens,
    temperature,
):
    """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–º"""
    if not base_model_name:
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"
    if not prompt:
        return "‚ùó –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å"

    try:
        tokenizer, model = load_inference_model(base_model_name, adapter_path)

        # –§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –∫–∞–∫ –¥–ª—è Qwen
        text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        inputs = tokenizer(text, return_tensors="pt").to("cuda")

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=int(max_new_tokens),
                temperature=float(temperature),
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return answer
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ: {str(e)}"


def get_progress():
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
    if training_progress:
        return "\n".join(training_progress[-50:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 —Å—Ç—Ä–æ–∫
    return "–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è..."


def check_status():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è"""
    if training_status["running"]:
        return "üîÑ –û–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è..."
    elif training_status["result"]:
        result = training_status["result"]
        if result.get("success"):
            return f"‚úÖ {result.get('message', '–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!')}"
        else:
            return f"‚ùå {result.get('message', '–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏')}"
    else:
        return "‚è∏ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞..."


# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
with gr.Blocks() as app:
    gr.Markdown("# üöÄ Fine-Tuning Assistant")
    gr.Markdown("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LoRA")
    
    with gr.Tabs():
        with gr.TabItem("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏"):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### –ú–æ–¥–µ–ª—å")
                    model_name = gr.Textbox(
                        label="–ò–º—è –º–æ–¥–µ–ª–∏ (HuggingFace ID –∏–ª–∏ –ø—É—Ç—å)",
                        value="Qwen/Qwen2.5-7B-Instruct",
                        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: Qwen/Qwen2.5-7B-Instruct –∏–ª–∏ ./my-model"
                    )
                    
                    gr.Markdown("### –î–∞—Ç–∞—Å–µ—Ç")
                    dataset_file = gr.File(
                        label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç (JSONL —Ñ–æ—Ä–º–∞—Ç)",
                        file_types=[".jsonl"],
                        type="filepath"
                    )
                    
                    gr.Markdown("### –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è")
                    output_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∞–≤—Ç–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)",
                        value="",
                        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: ./my-trained-model"
                    )

                    gr.Markdown("### –ê–¥–∞–ø—Ç–µ—Ä")
                    adapter_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É LoRA-–∞–¥–∞–ø—Ç–µ—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                        value="",
                        placeholder="./qwen-marx-003721/lora_adapter"
                    )
                    continue_adapter = gr.Checkbox(
                        label="–î–æ–æ–±—É—á–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä (–∞ –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–π)",
                        value=False
                    )
                
                with gr.Column():
                    gr.Markdown("### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
                    max_length = gr.Slider(
                        label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                        minimum=128,
                        maximum=4096,
                        value=1024,
                        step=128
                    )
                    
                    quantization_bits = gr.Radio(
                        label="–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (BitsAndBytes)",
                        choices=["4", "8", "–ù–µ—Ç"],
                        value="4",
                        info="4-bit —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"
                    )
                    
                    use_double_quant = gr.Checkbox(
                        label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –¥–ª—è 4-bit)",
                        value=True
                    )
                    
                    batch_size = gr.Slider(
                        label="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞",
                        minimum=1,
                        maximum=8,
                        value=1,
                        step=1
                    )
                    
                    gradient_accumulation_steps = gr.Slider(
                        label="–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤",
                        minimum=1,
                        maximum=32,
                        value=4,
                        step=1,
                        info="–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = batch_size √ó gradient_accumulation_steps"
                    )
                    
                    learning_rate = gr.Number(
                        label="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è",
                        value=2e-4,
                        precision=6
                    )
                    
                    num_train_epochs = gr.Slider(
                        label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö",
                        minimum=1,
                        maximum=10,
                        value=3,
                        step=1
                    )
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA")
                    lora_r = gr.Slider(
                        label="LoRA Rank (r)",
                        minimum=4,
                        maximum=128,
                        value=16,
                        step=4
                    )
                    
                    lora_alpha = gr.Slider(
                        label="LoRA Alpha",
                        minimum=4,
                        maximum=128,
                        value=32,
                        step=4
                    )
                    
                    lora_dropout = gr.Slider(
                        label="LoRA Dropout",
                        minimum=0.0,
                        maximum=0.5,
                        value=0.05,
                        step=0.01
                    )
                
                with gr.Column():
                    gr.Markdown("### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                    save_steps = gr.Slider(
                        label="–°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤",
                        minimum=10,
                        maximum=500,
                        value=50,
                        step=10
                    )
                    
                    logging_steps = gr.Slider(
                        label="–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤",
                        minimum=1,
                        maximum=100,
                        value=10,
                        step=1
                    )
                    
                    use_gradient_checkpointing = gr.Checkbox(
                        label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Gradient Checkpointing",
                        value=True,
                        info="–≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"
                    )
                    
                    use_8bit_optimizer = gr.Checkbox(
                        label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä",
                        value=True,
                        info="–≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å"
                    )
        
        with gr.TabItem("üìä –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏"):
            gr.Markdown("### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏")
            estimate_btn = gr.Button("–û—Ü–µ–Ω–∏—Ç—å –ø–∞–º—è—Ç—å", variant="primary")
            memory_info = gr.Markdown("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏")
            
            estimate_btn.click(
                fn=lambda qb, *args: estimate_memory_requirements(
                    args[0],
                    None if qb == "–ù–µ—Ç" else int(qb),
                    *args[1:]
                ),
                inputs=[
                    quantization_bits,
                    model_name,
                    batch_size,
                    max_length,
                    gradient_accumulation_steps,
                    use_gradient_checkpointing,
                    use_8bit_optimizer
                ],
                outputs=memory_info
            )
        
        with gr.TabItem("üöÄ –û–±—É—á–µ–Ω–∏–µ"):
            gr.Markdown("### –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è")
            
            with gr.Row():
                start_btn = gr.Button("–ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ", variant="primary", size="lg")
                refresh_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", variant="secondary")
            
            status_text = gr.Textbox(
                label="–°—Ç–∞—Ç—É—Å",
                value="‚è∏ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞...",
                interactive=False
            )
            
            progress_log = gr.Textbox(
                label="–õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è",
                lines=20,
                max_lines=50,
                interactive=False,
                value="–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è..."
            )
            
            # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            def update_progress():
                return get_progress(), check_status()
            
            start_btn.click(
                fn=start_training,
                inputs=[
                    model_name,
                    dataset_file,
                    output_dir,
                    adapter_path,
                    continue_adapter,
                    max_length,
                    quantization_bits,
                    use_double_quant,
                    batch_size,
                    gradient_accumulation_steps,
                    learning_rate,
                    num_train_epochs,
                    lora_r,
                    lora_alpha,
                    lora_dropout,
                    save_steps,
                    logging_steps,
                    use_gradient_checkpointing,
                    use_8bit_optimizer
                ],
                outputs=status_text
            ).then(
                fn=update_progress,
                inputs=None,
                outputs=[progress_log, status_text]
            )
            
            # –ö–Ω–æ–ø–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            refresh_btn.click(
                fn=update_progress,
                inputs=None,
                outputs=[progress_log, status_text]
            )

            # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–∞–π–º–µ—Ä–∞
            # –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ Gradio –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ –º–µ—Ç–æ–¥–∞ .change() –¥–ª—è gr.Timer.
            # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –ª–æ–≥–∏ –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø–æ –∫–Ω–æ–ø–∫–µ '–û–±–Ω–æ–≤–∏—Ç—å'.

        with gr.TabItem("üí¨ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"):
            gr.Markdown("### –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∞–¥–∞–ø—Ç–µ—Ä–æ–º")

            with gr.Row():
                with gr.Column():
                    base_model_infer = gr.Textbox(
                        label="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (HuggingFace ID –∏–ª–∏ –ø—É—Ç—å)",
                        value="Qwen/Qwen2.5-7B-Instruct",
                    )
                    adapter_infer = gr.Textbox(
                        label="–ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä—É (LoRA)",
                        value="./qwen-marx-003721/lora_adapter",
                    )
                    max_new_tokens_infer = gr.Slider(
                        label="–ú–∞–∫—Å–∏–º—É–º –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤",
                        minimum=16,
                        maximum=512,
                        value=250,
                        step=16,
                    )
                    temperature_infer = gr.Slider(
                        label="Temperature",
                        minimum=0.1,
                        maximum=1.5,
                        value=0.7,
                        step=0.05,
                    )
                with gr.Column():
                    prompt_infer = gr.Textbox(
                        label="–í–æ–ø—Ä–æ—Å / –∑–∞–ø—Ä–æ—Å",
                        lines=5,
                        placeholder="–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏...",
                    )
                    run_btn = gr.Button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç", variant="primary")
                    output_infer = gr.Textbox(
                        label="–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏",
                        lines=10,
                        interactive=False,
                    )

            run_btn.click(
                fn=run_inference,
                inputs=[
                    base_model_infer,
                    adapter_infer,
                    prompt_infer,
                    max_new_tokens_infer,
                    temperature_infer,
                ],
                outputs=output_infer,
            )


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ GPU.")
    
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    print("üì± –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä –∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:7860")
    print("üí° –ï—Å–ª–∏ –ø–æ—Ä—Ç –∑–∞–Ω—è—Ç, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç –¥—Ä—É–≥–æ–π –ø–æ—Ä—Ç\n")
    
    app.launch(
        share=False,
        server_name="127.0.0.1",
        server_port=7860,
        inbrowser=True,
        theme=gr.themes.Soft()
    )
