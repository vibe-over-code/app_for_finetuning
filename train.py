import torch
import os
import json
from datetime import datetime
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset

# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==========
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct" 
DATASET_PATH = "marx_dataset.jsonl" # –§–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –º—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ —á–µ—Ä–µ–∑ Mistral
OUTPUT_DIR = f"./qwen-marx-{datetime.now().strftime('%H%M%S')}"
MAX_LENGTH = 1024 # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 10GB VRAM
# ===================================

def main():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {torch.cuda.get_device_name(0)}")

    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (—á—Ç–æ–±—ã –≤–ª–µ–∑—Ç—å –≤ 10 –ì–ë)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∫ PEFT
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    # –î–ª—è Qwen 2.5 –≤–∞–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    lora_config = LoraConfig(
        r=16, 
        lora_alpha=32,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    # –û–∂–∏–¥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç JSONL —Å –ø–æ–ª—è–º–∏ instruction –∏ output
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

    def tokenize_function(examples):
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Å—Ç–∏–ª–µ —á–∞—Ç–∞ Qwen
        texts = []
        for i in range(len(examples['instruction'])):
            text = f"<|im_start|>user\n{examples['instruction'][i]}<|im_end|>\n<|im_start|>assistant\n{examples['output'][i]}<|im_end|>"
            texts.append(text)
        
        return tokenizer(
            texts,
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length"
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )

    # 6. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ RTX 3080)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4, # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 4
        learning_rate=2e-4,
        num_train_epochs=3,
        logging_steps=10,
        fp16=True, # –ù–∞ 3080 –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bf16=True, –µ—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä—ã —Å–≤–µ–∂–∏–µ
        save_strategy="steps",
        save_steps=50,
        save_total_limit=2,
        optim="paged_adamw_8bit", # –û—á–µ–Ω—å –≤–∞–∂–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
        report_to="none"
    )

    # 7. –ó–∞–ø—É—Å–∫
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    )

    print("üõ† –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è...")
    trainer.train()

    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
    trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, "lora_adapter"))
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ê–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {OUTPUT_DIR}/lora_adapter")

if __name__ == "__main__":
    main()