"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏
"""
import torch
import os
import json
import gc
from datetime import datetime
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from memory_estimator import (
    estimate_model_memory,
    estimate_training_memory,
    get_available_memory,
    check_memory_sufficiency
)


class MemoryError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏"""
    pass


def create_bitsandbytes_config(quantization_bits=4, use_double_quant=True):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è bitsandbytes
    
    Args:
        quantization_bits: 4, 8 –±–∏—Ç –∏–ª–∏ None –¥–ª—è –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
        use_double_quant: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –¥–ª—è 4-bit)
    
    Returns:
        BitsAndBytesConfig –∏–ª–∏ None
    """
    if quantization_bits is None or quantization_bits == 0:
        return None
    elif quantization_bits == 4:
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=use_double_quant,
        )
    elif quantization_bits == 8:
        return BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_compute_dtype=torch.float16,
        )
    else:
        return None


def get_target_modules(model_name):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è LoRA –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
    
    Args:
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏
    
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π
    """
    model_lower = model_name.lower()
    
    # –û–±—â–∏–µ –º–æ–¥—É–ª–∏ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
    common_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    
    if "qwen" in model_lower or "llama" in model_lower or "mistral" in model_lower:
        return common_modules + ["gate_proj", "up_proj", "down_proj"]
    elif "phi" in model_lower:
        return common_modules + ["fc1", "fc2"]
    else:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –º–æ–¥—É–ª–∏
        return common_modules


def train_model(
    model_name,
    dataset_path,
    output_dir=None,
    max_length=1024,
    quantization_bits=4,
    use_double_quant=True,
    batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    save_steps=50,
    logging_steps=10,
    use_gradient_checkpointing=True,
    use_8bit_optimizer=True,
    progress_callback=None
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç—å
        dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É JSONL
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        quantization_bits: –ë–∏—Ç–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (4 –∏–ª–∏ 8)
        use_double_quant: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        gradient_accumulation_steps: –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        num_train_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        lora_r: –†–∞–Ω–≥ LoRA
        lora_alpha: Alpha –¥–ª—è LoRA
        lora_dropout: Dropout –¥–ª—è LoRA
        save_steps: –®–∞–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        logging_steps: –®–∞–≥–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        use_gradient_checkpointing: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient checkpointing
        use_8bit_optimizer: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –¢—Ä–µ–±—É–µ—Ç—Å—è GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
        
        device_name = torch.cuda.get_device_name(0)
        if progress_callback:
            progress_callback(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {device_name}")
        
        # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
        if progress_callback:
            progress_callback("üìä –û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏...")
        
        model_memory = estimate_model_memory(model_name, quantization_bits)
        if 'error' in model_memory:
            if progress_callback:
                progress_callback(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏: {model_memory['error']}")
        else:
            training_memory = estimate_training_memory(
                model_memory['model_memory_gb'],
                batch_size,
                max_length,
                gradient_accumulation_steps,
                use_gradient_checkpointing,
                use_8bit_optimizer
            )
            available_memory = get_available_memory()
            memory_check = check_memory_sufficiency(
                training_memory['total_memory_gb'],
                available_memory['available_gb']
            )
            
            if progress_callback:
                progress_callback(f"üìä –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏:\n"
                                f"  –ú–æ–¥–µ–ª—å: {model_memory['model_memory_gb']} GB\n"
                                f"  –û–±—É—á–µ–Ω–∏–µ: {training_memory['total_memory_gb']} GB\n"
                                f"  –î–æ—Å—Ç—É–ø–Ω–æ: {available_memory['available_gb']} GB\n"
                                f"  {memory_check['recommendation']}")
            
            if not memory_check['sufficient']:
                raise MemoryError(memory_check['recommendation'])
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
        bnb_config = create_bitsandbytes_config(quantization_bits, use_double_quant)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        if progress_callback:
            progress_callback(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {model_name}...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏
        if progress_callback:
            progress_callback(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        
        try:
            load_kwargs = {
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True
            }
            if bnb_config is not None:
                load_kwargs["quantization_config"] = bnb_config
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **load_kwargs
            )
        except torch.cuda.OutOfMemoryError as e:
            gc.collect()
            torch.cuda.empty_cache()
            raise MemoryError(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏. "
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å quantization_bits –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å. "
                f"–û—à–∏–±–∫–∞: {str(e)}"
            )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∫ PEFT
        if use_gradient_checkpointing:
            model.gradient_checkpointing_enable()
        
        if quantization_bits in [4, 8] and bnb_config is not None:
            model = prepare_model_for_kbit_training(model)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
        target_modules = get_target_modules(model_name)
        
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = get_peft_model(model, lora_config)
        
        if progress_callback:
            model.print_trainable_parameters()
            progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –∫ –æ–±—É—á–µ–Ω–∏—é")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if progress_callback:
            progress_callback(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path}...")
        
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
        
        dataset = load_dataset("json", data_files=dataset_path, split="train")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —á–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏
        def get_chat_template(model_name):
            model_lower = model_name.lower()
            if "qwen" in model_lower:
                return lambda inst, out: f"<|im_start|>user\n{inst}<|im_end|>\n<|im_start|>assistant\n{out}<|im_end|>"
            elif "llama" in model_lower:
                return lambda inst, out: f"<s>[INST] {inst} [/INST] {out} </s>"
            elif "mistral" in model_lower:
                return lambda inst, out: f"<s>[INST] {inst} [/INST] {out} </s>"
            else:
                return lambda inst, out: f"User: {inst}\nAssistant: {out}"
        
        chat_template = get_chat_template(model_name)
        
        def tokenize_function(examples):
            texts = []
            for i in range(len(examples['instruction'])):
                inst = examples['instruction'][i]
                out = examples.get('output', [''])[i] if 'output' in examples else ''
                text = chat_template(inst, out)
                texts.append(text)
            
            return tokenizer(
                texts,
                truncation=True,
                max_length=max_length,
                padding="max_length"
            )
        
        if progress_callback:
            progress_callback("üîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ output_dir
        if output_dir is None:
            model_short = model_name.split('/')[-1]
            output_dir = f"./{model_short}-{datetime.now().strftime('%H%M%S')}"
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            num_train_epochs=num_train_epochs,
            logging_steps=logging_steps,
            fp16=True,
            save_strategy="steps",
            save_steps=save_steps,
            save_total_limit=2,
            optim="paged_adamw_8bit" if use_8bit_optimizer else "adamw_torch",
            report_to="none",
            remove_unused_columns=False
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        )
        
        if progress_callback:
            progress_callback("üõ† –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è...")
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏
        try:
            trainer.train()
        except torch.cuda.OutOfMemoryError as e:
            gc.collect()
            torch.cuda.empty_cache()
            raise MemoryError(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. "
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å batch_size, max_length –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å gradient_accumulation_steps. "
                f"–û—à–∏–±–∫–∞: {str(e)}"
            )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
        if progress_callback:
            progress_callback(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {output_dir}...")
        
        adapter_path = os.path.join(output_dir, "lora_adapter")
        trainer.model.save_pretrained(adapter_path)
        tokenizer.save_pretrained(output_dir)
        
        result = {
            'success': True,
            'output_dir': output_dir,
            'adapter_path': adapter_path,
            'message': f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ê–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {adapter_path}"
        }
        
        if progress_callback:
            progress_callback(result['message'])
        
        return result
        
    except MemoryError as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏: {str(e)}")
        return {
            'success': False,
            'error': 'memory',
            'message': str(e)
        }
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return {
            'success': False,
            'error': 'other',
            'message': str(e)
        }
    finally:
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
